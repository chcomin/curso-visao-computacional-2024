{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo multimodal CLIP\n",
    "\n",
    "Modelos multimodais realizam o processamento de dados de múltiplas modalidades. Veremos especificamente sobre o processamento de texto e imagens, que são os chamados *visual language models*. \n",
    "\n",
    "Implementaremos o modelo CLIP, desenvolvido pela OpenAI.\n",
    "\n",
    "Referências:\n",
    "\n",
    "* [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)\n",
    "* [Repositório do Hugginface](https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_clip.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import get_dataset, collate_fn\n",
    "\n",
    "ds, _ = get_dataset('../data/oxford_pets', '../data/oxford_pets_captions.txt')\n",
    "dl = DataLoader(ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "imgs, texts = next(iter(dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extração de atributos de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import TextEncoder\n",
    "\n",
    "text_encoder = TextEncoder()\n",
    "features = text_encoder(texts)\n",
    "features.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extração de atributos de imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2048])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "image_encoder = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "image_encoder.fc = nn.Identity()\n",
    "features = image_encoder(imgs)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo multimodal\n",
    "\n",
    "Temos um codificador de imagens que extrai 2048 atributos para cada imagem de entrada e um codificador de texto que extrai 768 atributos para cada texto. Criaremos um modelo que calcula a similaridade entre esses atributos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4253, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Clip(nn.Module):\n",
    "\n",
    "    def __init__(self, image_encoder, text_encoder, img_dim, text_dim,\n",
    "                 temp=2.6592, dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        # Camadas de projeção\n",
    "        self.visual_projection = nn.Linear(img_dim, dim, bias=False)\n",
    "        self.text_projection = nn.Linear(text_dim, dim, bias=False)\n",
    "        # Parâmetro treinável responsável por reescalar os valores de similaridade\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(temp)) \n",
    "\n",
    "    def project_images(self, imgs):\n",
    "        '''Codifica imagens.'''\n",
    "\n",
    "        image_embeds = self.image_encoder(imgs)\n",
    "        image_embeds = self.visual_projection(image_embeds)\n",
    "        # Normaliza os valores pela magnitude do vetor, que é a raiz quadrada \n",
    "        # da soma dos valores ao quadrado\n",
    "        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "        return image_embeds\n",
    "    \n",
    "    def project_texts(self, texts):\n",
    "        '''Codifica textos.'''\n",
    "\n",
    "        text_embeds = self.text_encoder(texts)\n",
    "        text_embeds = self.text_projection(text_embeds)\n",
    "        text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "        return text_embeds\n",
    "\n",
    "    def forward(self, imgs, texts, return_emb=False):\n",
    "\n",
    "        image_embeds = self.project_images(imgs)\n",
    "        text_embeds = self.project_texts(texts)\n",
    "        \n",
    "        logit_scale = self.logit_scale.exp()\n",
    "\n",
    "        # Similaridade de coseno. Cada linha i dessa matriz representa a \n",
    "        # similaridade entre o texto i e as imagens do batch. O elemento\n",
    "        # (i,i) representa a similaridade entre o texto i e a imagem correta \n",
    "        # que corresponde a esse texto, enquanto que os demais elementos da \n",
    "        # linha correspondem a correspondências incorretas. \n",
    "        # text_embeds: bs x dim\n",
    "        # image_embeds.t(): dim x \n",
    "        # logits_per_text: bs x bs\n",
    "        logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n",
    "\n",
    "        output = logits_per_text\n",
    "        # Opcionalmente, retorna as projeções das imagens e textos\n",
    "        if return_emb:\n",
    "            output += (image_embeds, text_embeds)\n",
    "\n",
    "        return output\n",
    "    \n",
    "def contrastive_loss(logits_per_text):\n",
    "    '''Calcula a entropia cruzada para cada linha da matriz, considerando\n",
    "    que a \"classe\" correta da linha i é dada pela coluna i.'''\n",
    "\n",
    "    scores = logits_per_text\n",
    "    targets = torch.arange(len(logits_per_text), device=logits_per_text.device)\n",
    "    loss = nn.functional.cross_entropy(scores, targets)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def clip_loss(similarity):\n",
    "    '''Queremos que a matriz de similaridade possua valores altos na diagonal,\n",
    "    e valores baixos fora da diagonal. Essa loss também é chamada de InfoNCE.'''\n",
    "\n",
    "    caption_loss = contrastive_loss(similarity)\n",
    "    image_loss = contrastive_loss(similarity.t())\n",
    "    return (caption_loss + image_loss) / 2.0\n",
    "\n",
    "# Se quisermos evitar de treinar o encoder de texto, podemos desabilitar\n",
    "# os gradientes\n",
    "text_encoder.requires_grad_(False)\n",
    "model = Clip(image_encoder, text_encoder, img_dim=2048, text_dim=768)\n",
    "\n",
    "logits_per_text = model(imgs, texts)\n",
    "loss = clip_loss(logits_per_text)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot accuracy\n",
    "\n",
    "Como o modelo envolve similaridade entre texto e imagens, é difícil medir a qualidade de forma intuitiva. Uma forma de fazer isso é verificando a capacidade do modelo de ser aplicado em outras tarefas como classificação de imagens. Como o modelo não foi treinado para tal tarefa, isso é chamado de zero-shot accuracy.\n",
    "\n",
    "Vamos implementar uma função simples de acurácia que verifica a capacidade do modelo em classificar as imagens nas classes gato e chachorro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def zero_shot_accuracy(model, imgs, texts, label_embeds, device):\n",
    "\n",
    "    # Estimativa da classe correta da imagem baseado na legenda. O ideal seria\n",
    "    # utilizar as classes conhecidas do dataset Oxford Pets, mas isso adicionaria\n",
    "    # complexidade ao código.\n",
    "    targets = []\n",
    "    for text in texts:\n",
    "        if 'cat' in text or 'kitten' in text:\n",
    "            target = 0\n",
    "        elif 'dog' in text or 'puppy' in text:\n",
    "            target = 1\n",
    "        else:\n",
    "            # Classe não reconhecida\n",
    "            target = 2\n",
    "        targets.append(target)\n",
    "    targets = torch.tensor(targets, device=device)\n",
    "\n",
    "    # Projeção das imagens\n",
    "    image_embeds = model.project_images(imgs)\n",
    "    # Similaridade entre cada imagem e as palavras 'cat' e 'dog'\n",
    "    scores = torch.matmul(image_embeds, label_embeds.t())\n",
    "    # Índice da classe mais provável\n",
    "    predictions = scores.argmax(dim=1)\n",
    "    # Fração das imagens da classe gato (cachorro) que são mais similares à \n",
    "    # palavra 'cat' ('dog')\n",
    "    mask = targets!=2\n",
    "    targets = targets[mask]\n",
    "    predictions = predictions[mask]\n",
    "    acc = (predictions==targets).float().mean()\n",
    "\n",
    "    return acc\n",
    "\n",
    "label_embeds = model.project_texts(['cat', 'dog'])\n",
    "zero_shot_accuracy(model, imgs, texts, label_embeds, 'cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
