{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance em GPU\n",
    "\n",
    "Veremos algumas estratégias para melhorar a perfomance em GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precisão numérica\n",
    "\n",
    "GPUs modernas realizam operações de forma muito mais eficiente em float16. Mas operações em float16 possuem menor precisão, então é preciso tomar cuidado com a estabilidade numérica dos resultados. Existem algumas abordagens para realizar operações em menor precisão e ao mesmo tempo evitar erros numéricos. \n",
    "\n",
    "Compararemos as seguintes situações que possuem diferentes relações custo x precisão:\n",
    "\n",
    "1. Operações em float64, o que dá o resultado mais preciso possível, mas é menos eficiente\n",
    "2. Operações em float32, que é o padrão do Pytorch\n",
    "3. O Pytorch disponibiliza a chamada *automatic mixed precision*. Quando é detectado que uma operação pode ser realizada sem muita perda de precisão, o Pytorch automaticamente realiza a operação em menor precisão. Para isso, é usado o contexto `torch.autocast`\n",
    "4. Realizar operações em float16, o que é extremamente eficiente em GPUs modernas. Mas é preciso tomar cuidado, por exemplo, não é recomendado realizar o backpropagation em float16\n",
    "5. Em GPUs recentes o Pytorch possui a função `torch.set_float32_matmul_precision`, que permite o uso de um tipo especial de dado, o chamado *tensorfloat32*. Esse tipo de dado é usado exclusivamente por tensor cores. Ele permite uma precisão próxima de float32 mas com a eficiência de float16\n",
    "\n",
    "Aplicaremos essas técnicas em multiplicações matriciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def benchmark(shape, dtype, n=5, n_warm=2):\n",
    "    '''Realiza `n` multiplicações matriciais entre matrizes de tamanho\n",
    "    shape[0]xshape[1] x shape[1]xshape[0]. Retorna a soma dos valores do resultado\n",
    "    e o tempo médio de cada multiplicação.'''\n",
    "\n",
    "    gpu_start = torch.cuda.Event(enable_timing=True)\n",
    "    gpu_end = torch.cuda.Event(enable_timing=True)  \n",
    "\n",
    "    nr, nc = shape\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    x1 = torch.randn(nr, nc, dtype=dtype, device='cuda')\n",
    "    x2 = torch.randn(nc, nr, dtype=dtype, device='cuda')\n",
    "\n",
    "    for _ in range(n_warm):\n",
    "        r = torch.matmul(x1, x2)\n",
    "\n",
    "    gpu_start.record() \n",
    "    for _ in range(n):\n",
    "        r = torch.matmul(x1, x2)\n",
    "    gpu_end.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t_gpu = gpu_start.elapsed_time(gpu_end)\n",
    "    res = r.abs().mean().item()\n",
    "    \n",
    "    return res, t_gpu/n\n",
    "\n",
    "# Memória disponível na GPU em GiB\n",
    "mem_size = 12\n",
    "# Quantidade de valores que podem ser alocados em float64\n",
    "nv = (12-1)*2**30//8\n",
    "# /2 porque vamos alocar duas matrizes\n",
    "nv = nv//2\n",
    "# Tamanho das matrizes. As dimensões terem ao menos tamanho 512 garante que\n",
    "# a GPU será utilizada ao máximo\n",
    "mat_shape = (512, nv//512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempos (ms):\n",
      "float64: 1395.7\n",
      "float32: 42.0\n",
      "float16: 12.2\n",
      "\n",
      "Resultados:\n",
      "float64: 955.571\n",
      "float32: 957.816\n",
      "float16: 957.500\n"
     ]
    }
   ],
   "source": [
    "# float64\n",
    "r_f64, t_f64 = benchmark(mat_shape, torch.float64)\n",
    "\n",
    "# float32\n",
    "r_f32, t_f32 = benchmark(mat_shape, torch.float32)\n",
    "\n",
    "# float16\n",
    "r_f16, t_f16 = benchmark(mat_shape, torch.float16)\n",
    "\n",
    "print('Tempos (ms):')\n",
    "print(f'float64: {t_f64:.1f}\\nfloat32: {t_f32:.1f}\\nfloat16: {t_f16:.1f}')\n",
    "\n",
    "print('\\nResultados:')\n",
    "print(f'float64: {r_f64:.3f}\\nfloat32: {r_f32:.3f}\\nfloat16: {r_f16:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note as diferenças de tempo. Os cálculos demoram muito mais em float64, e a multiplicação em float16 é mais de 3x mais rápida do que em float32! Isso representa um potencial de speedup de mais de 3x somente modificando a precisão! Mas note que os resultados apresentam pequenas diferenças.\n",
    "\n",
    "O Pytorch possui duas abordagens para realizar cálculos em meia precisão :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempos (ms):\n",
      "autocast: 23.3\n",
      "tensorfloat32: 24.2\n",
      "\n",
      "Resultados:\n",
      "autocast: 957.500\n",
      "tensorfloat32: 957.608\n"
     ]
    }
   ],
   "source": [
    "# Autocast para float16, em operações selecionadas. Multiplicação matricial\n",
    "# é uma dessas operações\n",
    "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "    r_af16, t_af16 = benchmark(mat_shape, torch.float32)\n",
    "\n",
    "# Uso do formato tf32 (tensorfloat32) para realizar a multiplicação\n",
    "torch.set_float32_matmul_precision('high')\n",
    "r_tf32, t_tf32 = benchmark(mat_shape, torch.float32)\n",
    "\n",
    "print('Tempos (ms):')\n",
    "print(f'autocast: {t_af16:.1f}\\ntensorfloat32: {t_tf32:.1f}')\n",
    "\n",
    "print('\\nResultados:')\n",
    "print(f'autocast: {r_af16:.3f}\\ntensorfloat32: {r_tf32:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O autocast e o formato tf32 permitem um speedup de quase 2x em relação à float32. \n",
    "\n",
    "Há também o formato bfloat32 que possui a mesma magnitude que float32 mas menos resolução. As informações sobre os formatos do Pytorch são:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info.max=3.4028234663852886e+38, info.smallest_normal=1.1754943508222875e-38, info.eps=1.1920928955078125e-07\n",
      "info.max=65504.0, info.smallest_normal=6.103515625e-05, info.eps=0.0009765625\n",
      "info.max=3.3895313892515355e+38, info.smallest_normal=1.1754943508222875e-38, info.eps=0.0078125\n"
     ]
    }
   ],
   "source": [
    "def pformat(format):\n",
    "    info = torch.finfo(format)\n",
    "    print(f'{info.max=}, {info.smallest_normal=}, {info.eps=}')\n",
    "\n",
    "pformat(torch.float32)\n",
    "pformat(torch.float16)\n",
    "pformat(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memória *page-locked*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempos (ms):\n",
      "pin=False: 237.1\n",
      "pin=True: 119.5\n"
     ]
    }
   ],
   "source": [
    "def copy(mat_shape, pin):\n",
    "\n",
    "    gpu_start = torch.cuda.Event(enable_timing=True)\n",
    "    gpu_end = torch.cuda.Event(enable_timing=True)  \n",
    "\n",
    "    x = torch.rand(mat_shape)\n",
    "    if pin:\n",
    "        x = x.pin_memory()\n",
    "\n",
    "    gpu_start.record()\n",
    "    x = x.to('cuda')\n",
    "    gpu_end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    t_gpu = gpu_start.elapsed_time(gpu_end)\n",
    "    \n",
    "    return t_gpu\n",
    "\n",
    "#mat_shape = (mat_shape[])\n",
    "t_nopin = copy(mat_shape, False)\n",
    "t_pin = copy(mat_shape, True)\n",
    "\n",
    "print('Tempos (ms):')\n",
    "print(f'pin=False: {t_nopin:.1f}\\npin=True: {t_pin:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiar tensores com pin_memory habilitado é quase 2x mais rápido. Isso é muito relevante, considerando que copiar dados da CPU para a GPU é uma operação extremamente custosa (note que demora mais para copiar uma matriz do que para realizar a multiplicação matricial entre centenas de milhões de valores).\n",
    "\n",
    "Os dataloaders do Pytorch possuem uma parâmetro `pin_memory` que quando True utiliza essa técnica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilação do grafo de execução\n",
    "\n",
    "Cada operação realizada pelo Pytorch na GPU envolve a chamada de uma função cuda. Isso representa um custo adicional de comunicação entre CPU e GPU. Adicionalmente, existem diversas operações que podem ser otimizadas. Por exemplo, uma operação de convolução seguida de uma operação batchnorm pode ser representada por uma única operação linear dado que ambas as operações são lineares. Um loop de treinamento envolve chamar exatamente as mesma funções diversas vezes. O Pytorch permite compilar um grafo de executação que otimizará uma sequência de operações realizadas na GPU. Isso é feito através da função `torch.compile()`. Por exemplo, dado um modelo, podemos fazer:\n",
    "\n",
    "`model = torch.compile(model, fullgraph=True, dynamic=False)`\n",
    "\n",
    "O modelo pode então ser utilizado no loop de treinamento. Isso pode levar a speedups significativos. Mas há restrições nos tipos de modelos que podem ser compilados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
