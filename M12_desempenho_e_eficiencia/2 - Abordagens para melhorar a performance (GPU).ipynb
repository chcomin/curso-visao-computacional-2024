{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance em GPU [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/chcomin/curso-visao-computacional-2024/blob/main/M12_desempenho_e_eficiencia/2%20-%20Abordagens%20para%20melhorar%20a%20performance%20(GPU).ipynb)\n",
    "\n",
    "Veremos algumas estratégias para melhorar a perfomance em GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precisão numérica\n",
    "\n",
    "GPUs modernas realizam operações de forma muito mais eficiente em float16. Mas operações em float16 possuem menor precisão, então é preciso tomar cuidado com a estabilidade numérica dos resultados. Existem algumas abordagens para realizar operações em menor precisão e ao mesmo tempo evitar erros numéricos. \n",
    "\n",
    "Compararemos as seguintes situações que possuem diferentes relações custo x precisão:\n",
    "\n",
    "1. Operações em float64, o que dá o resultado mais preciso possível, mas é menos eficiente\n",
    "2. Operações em float32, que é o padrão do Pytorch\n",
    "3. O Pytorch disponibiliza a chamada *automatic mixed precision*. Quando é detectado que uma operação pode ser realizada sem muita perda de precisão, o Pytorch automaticamente realiza a operação em menor precisão. Para isso, é usado o contexto `torch.autocast`\n",
    "4. Realizar operações em float16, o que é extremamente eficiente em GPUs modernas. Mas é preciso tomar cuidado, por exemplo, não é recomendado realizar o backpropagation em float16\n",
    "5. Em GPUs recentes o Pytorch possui a função `torch.set_float32_matmul_precision`, que permite o uso de um tipo especial de dado, o chamado *tensorfloat32*. Esse tipo de dado é usado exclusivamente por tensor cores. Ele permite uma precisão próxima de float32 mas com a eficiência de float16\n",
    "\n",
    "Aplicaremos essas técnicas em multiplicações matriciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class PerfRecorder:\n",
    "    \"\"\"Registra o tempo de execução na GPU e o uso de memória.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.gpu_start = torch.cuda.Event(enable_timing=True)\n",
    "        self.gpu_end = torch.cuda.Event(enable_timing=True)  \n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Inicia registro.\"\"\"\n",
    "\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        self.gpu_start.record() \n",
    "\n",
    "    def end(self):\n",
    "        \"\"\"Encerra registro.\"\"\"\n",
    "\n",
    "        self.gpu_end.record()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # Tempo de execução na GPU, em ms\n",
    "        t_gpu = self.gpu_start.elapsed_time(self.gpu_end)\n",
    "        # Uso de memória em GiB\n",
    "        max_memory = torch.cuda.max_memory_allocated()/2**30\n",
    "    \n",
    "        return t_gpu, max_memory\n",
    "    \n",
    "def benchmark(shape, dtype, n=5, n_warm=2):\n",
    "    '''Realiza `n` multiplicações matriciais entre matrizes de tamanho\n",
    "    shape[0]xshape[1] x shape[1]xshape[0]. Retorna o tempo médio de cada \n",
    "    multiplicação, a memória utilizada e a média dos valores do resultado.'''\n",
    "\n",
    "    recorder = PerfRecorder()\n",
    "\n",
    "    nr, nc = shape\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    x1 = torch.randn(nr, nc, dtype=dtype, device='cuda')\n",
    "    x2 = torch.randn(nc, nr, dtype=dtype, device='cuda')\n",
    "\n",
    "    for _ in range(n_warm):\n",
    "        _ = torch.matmul(x1, x2)\n",
    "\n",
    "    recorder.start()\n",
    "    for _ in range(n):\n",
    "        r = torch.matmul(x1, x2)\n",
    "    t_gpu, max_memory = recorder.end()\n",
    "    t_gpu /= n\n",
    "\n",
    "    # Média dos valores do resultado do cálculo\n",
    "    mean_val = r.abs().mean().item()\n",
    "    \n",
    "    return t_gpu, max_memory, mean_val\n",
    "\n",
    "# Memória disponível na GPU em GiB\n",
    "mem_size = 12\n",
    "# Quantidade de valores que podem ser alocados em float64, excluindo 2 GiB para\n",
    "# evitar problemas de memória\n",
    "nv = (mem_size-2)*2**30//8\n",
    "# //2 porque vamos alocar duas matrizes\n",
    "nv = nv//2\n",
    "# Tamanho das matrizes. As dimensões terem ao menos tamanho 512 garante que\n",
    "# a GPU será utilizada ao máximo\n",
    "mat_shape = (512, nv//512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempos (ms):\n",
      "float64: 1274.5\n",
      "float32: 38.2\n",
      "float16: 11.1\n",
      "\n",
      "Memória (GiB):\n",
      "float64: 10.0\n",
      "float32: 5.0\n",
      "float16: 2.5\n",
      "\n",
      "Média dos valores:\n",
      "float64: 913.142\n",
      "float32: 914.738\n",
      "float16: 912.500\n"
     ]
    }
   ],
   "source": [
    "# float64\n",
    "t_f64, m_f64, v_f64 = benchmark(mat_shape, torch.float64)\n",
    "# float32\n",
    "t_f32, m_f32, v_f32 = benchmark(mat_shape, torch.float32)\n",
    "# float16\n",
    "t_f16, m_f16, v_f16 = benchmark(mat_shape, torch.float16)\n",
    "\n",
    "print('Tempos (ms):')\n",
    "print(f'float64: {t_f64:.1f}\\nfloat32: {t_f32:.1f}\\nfloat16: {t_f16:.1f}')\n",
    "\n",
    "print('\\nMemória (GiB):')\n",
    "print(f'float64: {m_f64:.1f}\\nfloat32: {m_f32:.1f}\\nfloat16: {m_f16:.1f}')\n",
    "\n",
    "print('\\nMédia dos valores:')\n",
    "print(f'float64: {v_f64:.3f}\\nfloat32: {v_f32:.3f}\\nfloat16: {v_f16:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note as diferenças de tempo. Os cálculos demoram muito mais em float64, e a multiplicação em float16 é mais de 3x mais rápida do que em float32! Isso representa um potencial de speedup de mais de 3x somente modificando a precisão! Mas note que os resultados apresentam pequenas diferenças.\n",
    "\n",
    "O Pytorch possui duas abordagens para realizar cálculos em meia precisão :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempos (ms):\n",
      "autocast: 21.2\n",
      "tensorfloat32: 22.2\n",
      "\n",
      "Memória (GiB):\n",
      "autocast: 7.5\n",
      "tensorfloat32: 5.0\n",
      "\n",
      "Resultados:\n",
      "autocast: 914.500\n",
      "tensorfloat32: 914.558\n"
     ]
    }
   ],
   "source": [
    "# Autocast para float16, em operações selecionadas. Multiplicação matricial\n",
    "# é uma dessas operações\n",
    "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "    t_af16, m_af16, v_af16 = benchmark(mat_shape, torch.float32)\n",
    "\n",
    "# Uso do formato tf32 (tensorfloat32) para realizar a multiplicação\n",
    "torch.set_float32_matmul_precision('high')\n",
    "t_tf32, m_tf32, v_tf32 = benchmark(mat_shape, torch.float32)\n",
    "\n",
    "print('Tempos (ms):')\n",
    "print(f'autocast: {t_af16:.1f}\\ntensorfloat32: {t_tf32:.1f}')\n",
    "\n",
    "print('\\nMemória (GiB):')\n",
    "print(f'autocast: {m_af16:.1f}\\ntensorfloat32: {m_tf32:.1f}')\n",
    "\n",
    "print('\\nResultados:')\n",
    "print(f'autocast: {v_af16:.3f}\\ntensorfloat32: {v_tf32:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O autocast e o formato tf32 permitem um speedup em relação à float32. \n",
    "\n",
    "Há também o formato bfloat32 que possui a mesma magnitude que float32 mas menos resolução. As informações sobre os formatos do Pytorch são:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info.max=3.4028234663852886e+38, info.smallest_normal=1.1754943508222875e-38, info.eps=1.1920928955078125e-07\n",
      "info.max=65504.0, info.smallest_normal=6.103515625e-05, info.eps=0.0009765625\n",
      "info.max=3.3895313892515355e+38, info.smallest_normal=1.1754943508222875e-38, info.eps=0.0078125\n"
     ]
    }
   ],
   "source": [
    "def pformat(format):\n",
    "    info = torch.finfo(format)\n",
    "    print(f'{info.max=}, {info.smallest_normal=}, {info.eps=}')\n",
    "\n",
    "pformat(torch.float32)\n",
    "pformat(torch.float16)\n",
    "pformat(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memória *page-locked*\n",
    "\n",
    "Dados copiados da CPU para a GPU não podem residir em memória pageada. Por padrão, um tensor do Pytorch é alocado em memória pageada. É possível criar um tensor em um intervalo de memória não pageada pelo sistema. Esse processo é denominado de \"pinned memory\", ou page-locked. Ele aumenta de forma significativa o desempenho da cópia dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempos (ms):\n",
      "pin=False: 213.9\n",
      "pin=True: 108.8\n"
     ]
    }
   ],
   "source": [
    "def copy(mat_shape, pin):\n",
    "\n",
    "    recorder = PerfRecorder()\n",
    "\n",
    "    x = torch.rand(mat_shape)\n",
    "    if pin:\n",
    "        x = x.pin_memory()\n",
    "\n",
    "    recorder.start()\n",
    "    x = x.to('cuda')\n",
    "    t_gpu, _ = recorder.end()\n",
    "    \n",
    "    return t_gpu\n",
    "\n",
    "t_nopin = copy(mat_shape, False)\n",
    "t_pin = copy(mat_shape, True)\n",
    "\n",
    "print('Tempos (ms):')\n",
    "print(f'pin=False: {t_nopin:.1f}\\npin=True: {t_pin:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiar tensores com pin_memory habilitado é quase 2x mais rápido. Isso é muito relevante, considerando que copiar dados da CPU para a GPU é uma operação extremamente custosa (note que demora mais para copiar uma matriz do que para realizar a multiplicação matricial entre centenas de milhões de valores).\n",
    "\n",
    "Os dataloaders do Pytorch possuem uma parâmetro `pin_memory` que quando True utiliza essa técnica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint de gradiente\n",
    "\n",
    "Checkpoint de gradiente é uma técnica que permite utilizar menos memória da GPU em troca de um custo computacional um pouco maior. Uma ou mais camadas do modelo são definidas como *checkpoints*. No processamento direto (forward), a ativação de uma camada checkpoint é salva, e as ativações das camadas seguintes não são salvas no grafo de computação. No momento do cálculo de gradientes, as ativações a partir da camada checkpoint são recalculadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo base:\n",
      "Tempo (ms): 955.8\n",
      "Memória (GiB): 8.2\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50\n",
    "\n",
    "def bench_model(model, bs, n=5, n_warm=2):\n",
    "    \"\"\"Mede o tempo de execução e uso de memória de um loop de treinamento.\"\"\"\n",
    "\n",
    "    optim = torch.optim.SGD(model.parameters())\n",
    "    x = torch.rand(bs, 3, 224, 224, device='cuda')\n",
    "    recorder = PerfRecorder()\n",
    "\n",
    "    for _ in range(n_warm):\n",
    "        _ = model(x)\n",
    "\n",
    "    recorder.start()\n",
    "    for _ in range(n):\n",
    "        scores = model(x)\n",
    "        loss = scores.mean()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    t_gpu, max_memory = recorder.end()\n",
    "\n",
    "    return t_gpu, max_memory\n",
    "\n",
    "model = resnet50().to('cuda')\n",
    "\n",
    "t_resnet, m_resnet = bench_model(model, bs=96)\n",
    "print('Modelo base:')\n",
    "print(f'Tempo (ms): {t_resnet:.1f}\\nMemória (GiB): {m_resnet:.1f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo com checkpoint\n",
      "Tempo (ms): 1124.6\n",
      "Memória (GiB): 3.6\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class ResNetChkp(nn.Module):\n",
    "    \"\"\"Cria um modelo ResNet utilizando a técnica de checkpoint de gradiente.\"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "\n",
    "        # Grupos de camadas da ResNet. Os primeiros dois grupos possuem \n",
    "        # ativações de alta resolução\n",
    "        self.group1 = nn.Sequential(\n",
    "            model.conv1,\n",
    "            model.bn1,\n",
    "            model.relu,\n",
    "            model.maxpool,\n",
    "            model.layer1[:2]\n",
    "        )\n",
    "        self.group2 = nn.Sequential(\n",
    "            model.layer1[2:],\n",
    "            model.layer2[:2]\n",
    "        )\n",
    "        self.group3 = nn.Sequential(\n",
    "            model.layer2[2:],\n",
    "            model.layer3,\n",
    "            model.layer4,\n",
    "            model.avgpool    \n",
    "        )\n",
    "        self.fc = model.fc\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Aplica grupos 1 e 2 utilizando checkpoint. As ativações das camadas não\n",
    "        # serão salvas no grafo de computação. Quando elas forem necessárias,\n",
    "        # serão recalculadas através de uma nova aplicação das camadas na entrada\n",
    "        x = checkpoint(self.group1, x, use_reentrant=False)\n",
    "        x = checkpoint(self.group2, x, use_reentrant=False)\n",
    "        x = self.group3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "resnet_ckp = ResNetChkp(model)\n",
    "\n",
    "t_resnet, m_resnet = bench_model(resnet_ckp, bs=96)\n",
    "print('Modelo com checkpoint')\n",
    "print(f'Tempo (ms): {t_resnet:.1f}\\nMemória (GiB): {m_resnet:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo com checkpoint utiliza 43% da memória do modelo original e possui tempo de processamento 15% maior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilação do grafo de execução\n",
    "\n",
    "Cada operação realizada pelo Pytorch na GPU envolve a chamada de uma função cuda. Isso representa um custo adicional de comunicação entre CPU e GPU. Adicionalmente, existem diversas operações que podem ser otimizadas. Por exemplo, uma operação de convolução seguida de uma operação batchnorm pode ser representada por uma única operação linear dado que ambas as operações são lineares. Um loop de treinamento envolve chamar exatamente as mesma funções diversas vezes. O Pytorch permite compilar um grafo de executação que otimizará uma sequência de operações realizadas na GPU. Isso é feito através da função `torch.compile()`. Por exemplo, dado um modelo, podemos fazer:\n",
    "\n",
    "`model = torch.compile(model, fullgraph=True, dynamic=False)`\n",
    "\n",
    "O modelo pode então ser utilizado no loop de treinamento. Isso pode levar a speedups significativos. Mas há restrições nos tipos de modelos que podem ser compilados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
