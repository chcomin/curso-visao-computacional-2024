{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estabilizando rede neurais - Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problemas com redes profundas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layers): ModuleList(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (13): ReLU()\n",
       "    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (15): ReLU()\n",
       "    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (17): ReLU()\n",
       "    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (19): ReLU()\n",
       "  )\n",
       "  (pool): AdaptiveMaxPool2d(output_size=2)\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def conv3x3(in_channels, out_channels):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    '''Modelo com n camadas convolucionais.'''\n",
    "    def __init__(self, num_layers, features):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [\n",
    "            conv3x3(1,features),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        for idx in range(1, num_layers):\n",
    "            layers.append(conv3x3(features,features))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "        self.pool = nn.AdaptiveMaxPool2d(2)\n",
    "        # Camada linear que recebe features imagens de tamanho 2x2 e gera 10 valores de saída\n",
    "        self.fc = nn.Linear(features*2*2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        acts = []\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            acts.append(x)\n",
    "        x = self.pool(x)\n",
    "        # Transformação das imagens de tamanho bs x features x 2 x 2 para bs x features*2*2\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x, acts\n",
    "    \n",
    "model = Model(10, 16)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a célula abaixo diversas vezes com ctrl+enter. A saída é sempre a mesma!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8473, 0.6993, 0.6797, 0.8377, 0.5288, 0.7625, 0.5635, 0.6401, 0.9126,\n",
      "        0.3690, 0.0333, 0.6792, 0.2206, 0.6391, 0.2469, 0.9062, 0.0577, 0.1945,\n",
      "        0.7032, 0.8367, 0.5065, 0.3833, 0.4205, 0.3516, 0.0329, 0.7910, 0.2805,\n",
      "        0.3771])\n",
      "tensor([[ 0.0629, -0.0275,  0.0358, -0.0511,  0.0846, -0.0625, -0.0376, -0.0450,\n",
      "          0.0303, -0.0785]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 1, 28, 28)\n",
    "y, acts = model(x)\n",
    "\n",
    "print(x[0,0,0])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A saída da rede é dada apenas pelo bias da última camada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0629, -0.0275,  0.0358, -0.0511,  0.0846, -0.0625, -0.0376, -0.0450,\n",
      "          0.0303, -0.0785]], grad_fn=<AddmmBackward0>)\n",
      "tensor([ 0.0629, -0.0275,  0.0359, -0.0511,  0.0845, -0.0624, -0.0377, -0.0450,\n",
      "         0.0303, -0.0786])\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(model.fc.bias.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotando a média e desvio padrão das ativações intermediárias da rede, vemos que os valores caem para zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 320x240 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_activations(acts):\n",
    "    act_mean = []\n",
    "    act_std = []\n",
    "    for act in acts:\n",
    "        act = act.detach()\n",
    "        act_mean.append(act.mean())\n",
    "        act_std.append(act.std())\n",
    "\n",
    "    plt.plot(act_mean)\n",
    "    plt.plot(act_std)\n",
    "\n",
    "show_activations(acts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essencialmente, a operação que uma rede neural faz é dada por:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.6831e+30, 2.5841e+30, 3.2441e+30, 3.3194e+30, 3.0015e+30, 3.2231e+30,\n",
      "        3.4129e+30, 2.5755e+30, 3.1955e+30, 3.2483e+30])\n"
     ]
    }
   ],
   "source": [
    "# Batch de dados\n",
    "x = torch.rand(10)\n",
    "# Parâmetros das camadas\n",
    "weight = 2.\n",
    "bias = 2.\n",
    "# 100 camadas\n",
    "for i in range(100):\n",
    "    x = (weight*x+bias)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tente achar valores de `weight` e `bias` que não levem o valor de x acima a 0, infinito ou algum valor trivial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao normalizarmos os valores, as ativações se tornam bem comportadas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1140,  0.1620, -0.7996, -1.3525, -0.1127,  0.4383, -1.4303,  1.7541,\n",
      "         0.9583,  0.4964])\n"
     ]
    }
   ],
   "source": [
    "def normalization(x):\n",
    "    mean = x.mean()\n",
    "    std = x.std()\n",
    "    return (x-mean)/std\n",
    "\n",
    "x = torch.rand(10)\n",
    "weight = 20.\n",
    "bias = -10.\n",
    "for i in range(10):\n",
    "    x = (weight*x+bias)\n",
    "    x = normalization(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas e se a escala dos valores for relevante para a classificação? Por exemplo, talvez um filtro tenda a gerar valores em torno de 10, enquanto que outro filtro gere valores em torno de 0.1. Essa informação será perdida após a normalização.\n",
    "\n",
    "Uma camada batchnorm permite reescalar os valores caso seja necessário. Os parâmetros de escala são aprendidos durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9673,  0.0576, -0.4464,  1.7995,  0.0592,  0.4724, -0.8892, -0.0076,\n",
       "         1.3030, -1.3814], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # nn.Parameter define um tensor com requires_grad=True e que é registrado \n",
    "        # como parâmetro treinável do modelo\n",
    "        self.gamma = nn.Parameter(torch.tensor(1.))\n",
    "        self.beta = nn.Parameter(torch.tensor(0.))\n",
    "\n",
    "        self.eps = 1e-5\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        mean = x.mean()\n",
    "        var = x.var()   # Variância\n",
    "        # eps evita divisão por 0\n",
    "        x_norm = (x-mean)/torch.sqrt(var+self.eps)\n",
    "\n",
    "        # Reescala os valores\n",
    "        y = self.gamma*x_norm + self.beta\n",
    "\n",
    "        return y\n",
    "    \n",
    "bn = BatchNorm()\n",
    "bn(100*torch.rand(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas temos um problema. E quando formos aplicar a rede após o treinamento em uma única imagem? Cada camada terá apenas uma única ativação, o que pode ser um problema. Para evitar isso, durante o treinamento podemos armazenar a média e desvio padrão de todos os batches, e usar a estimativa desses parâmetros de todos os batches para normalizar os valores ao aplicar em uma única imagem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desvendando a camada BatchNorm do Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Batchnorm aplicado em 3 canais\n",
    "bn = nn.BatchNorm1d(3)\n",
    "print(bn.weight)\n",
    "print(bn.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A longo de vários batches, a camada estima a média e variância dos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([0.3825, 0.4098, 0.4038])\n",
      "mean tensor([0.7556, 0.7789, 0.7328])\n",
      "mean tensor([1.0560, 1.1236, 1.0624])\n",
      "mean tensor([1.3408, 1.3960, 1.3496])\n",
      "mean tensor([1.6135, 1.6755, 1.6234])\n",
      "mean tensor([1.8706, 1.9412, 1.8425])\n",
      "mean tensor([2.0712, 2.1825, 2.0699])\n",
      "mean tensor([2.2403, 2.3370, 2.2614])\n",
      "mean tensor([2.4404, 2.4933, 2.4453])\n",
      "mean tensor([2.6201, 2.6510, 2.5843])\n",
      "mean tensor([2.7939, 2.7864, 2.7396])\n",
      "mean tensor([2.9021, 2.8785, 2.8963])\n",
      "mean tensor([2.9716, 3.0013, 3.0188])\n",
      "mean tensor([3.0809, 3.1256, 3.1079])\n",
      "mean tensor([3.1545, 3.2160, 3.1582])\n",
      "mean tensor([3.2282, 3.3161, 3.2074])\n",
      "mean tensor([3.3134, 3.4059, 3.3017])\n",
      "mean tensor([3.3674, 3.4433, 3.3521])\n",
      "mean tensor([3.4590, 3.4982, 3.4109])\n",
      "mean tensor([3.5065, 3.5586, 3.4915])\n",
      "mean tensor([3.5676, 3.5787, 3.5603])\n",
      "mean tensor([3.6255, 3.6332, 3.6401])\n",
      "mean tensor([3.6514, 3.6598, 3.6531])\n",
      "mean tensor([3.6433, 3.6989, 3.7077])\n",
      "mean tensor([3.6612, 3.7635, 3.7399])\n",
      "mean tensor([3.6653, 3.7983, 3.7768])\n",
      "mean tensor([3.7056, 3.8104, 3.7758])\n",
      "mean tensor([3.7121, 3.8309, 3.8176])\n",
      "mean tensor([3.7386, 3.8563, 3.8606])\n",
      "mean tensor([3.7178, 3.8607, 3.8614])\n",
      "mean tensor([3.7395, 3.8927, 3.9102])\n",
      "mean tensor([3.7545, 3.9008, 3.9417])\n",
      "mean tensor([3.7917, 3.9227, 3.9108])\n",
      "mean tensor([3.7721, 3.9511, 3.9443])\n",
      "mean tensor([3.8161, 3.9341, 3.9405])\n",
      "mean tensor([3.8188, 3.9552, 3.9251])\n",
      "mean tensor([3.8351, 3.9344, 3.9464])\n",
      "mean tensor([3.8367, 3.9754, 3.9266])\n",
      "mean tensor([3.8886, 3.9813, 3.9342])\n",
      "mean tensor([3.9126, 3.9950, 3.9616])\n",
      "mean tensor([3.9249, 3.9861, 3.9711])\n",
      "mean tensor([3.9200, 3.9699, 3.9659])\n",
      "mean tensor([3.9068, 3.9642, 3.9886])\n",
      "mean tensor([3.8817, 3.9583, 3.9735])\n",
      "mean tensor([3.9402, 3.9166, 3.9784])\n",
      "mean tensor([3.9188, 3.9774, 3.9490])\n",
      "mean tensor([3.9431, 3.9626, 3.9693])\n",
      "mean tensor([3.9490, 4.0007, 3.9747])\n",
      "mean tensor([3.9733, 4.0018, 3.9919])\n",
      "mean tensor([3.9944, 4.0155, 3.9999])\n",
      "mean tensor([3.9905, 3.9977, 3.9826])\n",
      "mean tensor([4.0069, 4.0007, 4.0007])\n",
      "mean tensor([3.9809, 4.0074, 3.9493])\n",
      "mean tensor([3.9713, 3.9692, 3.9087])\n",
      "mean tensor([3.9736, 4.0121, 3.9233])\n",
      "mean tensor([3.9911, 3.9759, 3.9257])\n",
      "mean tensor([3.9980, 4.0062, 3.9266])\n",
      "mean tensor([4.0190, 4.0031, 3.9343])\n",
      "mean tensor([4.0214, 3.9683, 3.9209])\n",
      "mean tensor([4.0372, 3.9935, 3.9167])\n",
      "mean tensor([4.0093, 3.9938, 3.9422])\n",
      "mean tensor([3.9815, 4.0091, 3.9238])\n",
      "mean tensor([3.9786, 3.9931, 3.9553])\n",
      "mean tensor([3.9561, 3.9436, 3.9631])\n",
      "mean tensor([3.9590, 3.9890, 3.9908])\n",
      "mean tensor([3.9754, 3.9667, 4.0072])\n",
      "mean tensor([3.9624, 3.9684, 4.0226])\n",
      "mean tensor([3.9952, 3.9592, 4.0160])\n",
      "mean tensor([4.0031, 3.9497, 3.9547])\n",
      "mean tensor([4.0141, 3.9632, 3.9540])\n",
      "mean tensor([4.0170, 3.9893, 3.9342])\n",
      "mean tensor([4.0281, 4.0106, 3.9384])\n",
      "mean tensor([4.0105, 4.0251, 3.9548])\n",
      "mean tensor([3.9905, 4.0333, 3.9401])\n",
      "mean tensor([4.0218, 4.0304, 3.9614])\n",
      "mean tensor([4.0032, 4.0273, 3.9336])\n",
      "mean tensor([3.9805, 4.0100, 3.9063])\n",
      "mean tensor([3.9716, 4.0082, 3.9441])\n",
      "mean tensor([3.9876, 3.9902, 3.9080])\n",
      "mean tensor([3.9909, 4.0295, 3.8911])\n",
      "mean tensor([3.9814, 4.0383, 3.8919])\n",
      "mean tensor([3.9959, 3.9939, 3.9343])\n",
      "mean tensor([4.0195, 4.0076, 3.9339])\n",
      "mean tensor([4.0561, 4.0192, 3.9627])\n",
      "mean tensor([4.0577, 4.0052, 3.9760])\n",
      "mean tensor([4.0410, 4.0057, 3.9687])\n",
      "mean tensor([4.0691, 4.0032, 3.9525])\n",
      "mean tensor([4.0732, 4.0289, 3.9316])\n",
      "mean tensor([4.0692, 4.0244, 3.9079])\n",
      "mean tensor([4.0454, 4.0275, 3.9253])\n",
      "mean tensor([4.0640, 3.9904, 3.9151])\n",
      "mean tensor([4.0020, 3.9830, 3.9567])\n",
      "mean tensor([4.0146, 3.9985, 3.9536])\n",
      "mean tensor([4.0319, 3.9954, 3.9555])\n",
      "mean tensor([4.0215, 4.0008, 3.9419])\n",
      "mean tensor([4.0109, 3.9685, 3.9802])\n",
      "mean tensor([4.0055, 3.9656, 4.0127])\n",
      "mean tensor([4.0032, 3.9522, 4.0302])\n",
      "mean tensor([4.0119, 3.9742, 4.0107])\n",
      "mean tensor([4.0100, 3.9676, 4.0131])\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    # Batches de 4 elementos, 3 canais cada e 6 valores para cada canal\n",
    "    # Os valores gerados possuem média 4 e variância 9.\n",
    "    x = torch.normal(mean=4, std=1, size=(4,3,6))\n",
    "    _ = bn(x)\n",
    "    print('mean', bn.running_mean)\n",
    "    #print('var', bn.running_var)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No modo eval, a média estimada é usada quando a camada for aplicada em novos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.9520, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Batch de tamanho 1000 com valores de média 10\n",
    "# A média do resultado deve ser aproximadamente 10-bn.running_mean\n",
    "x = torch.normal(mean=10, std=6, size=(1000,3,60))\n",
    "bn.eval()\n",
    "y = bn(x)\n",
    "print(y.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retornando o batchnorm para o modo .train faz com que a camada volte a normalizar pela média e desvio do batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.5022e-09, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.normal(mean=10, std=6, size=(1000,3,60))\n",
    "bn.train()\n",
    "y = bn(x)\n",
    "print(y.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
