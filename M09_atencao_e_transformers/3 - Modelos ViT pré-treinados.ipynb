{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos ViT pré-treinados\n",
    "\n",
    "Ao contrário de CNNs, modelos ViT são extremamente difíceis de treinar \"do zero\" (a partir de pesos aleatórios). Por isso, é comum que esses modelos sejam inicializados a partir de pesos pré-treinados disponíveis. Uma hipótese atual, ainda sendo estudada, é que modelos pré-treinados em grandes quantidades de dados tendem a possuir melhor acurácia em tarefas *downstream* (classificação, segmentação, etc) do que modelos treinados em menos dados. \n",
    "\n",
    "Um paradigma atual de pré-treinamento de modelos é realizar o treinamento em dados **não rotulados**. Isso permite realizar o treinamento em uma quantidade massiva de dados.\n",
    "\n",
    "Este notebook mostra as funcionalidades da biblioteca timm para carregar modelos pré-treinados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biblioteca Pytorch Image Models (timm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "\n",
    "# Modelo ViT utilizando patches de tamanho 16x16 e imagens de entrada de\n",
    "# tamanho 224x224\n",
    "tag = 'vit_base_patch16_224'\n",
    "\n",
    "model = timm.create_model(tag, pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "x = torch.rand(1, 3, 224, 224)\n",
    "with torch.no_grad():\n",
    "    scores = model(x)\n",
    "\n",
    "# Scores associados a 1000 classes do ImageNet\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível cerregarmos o modelo como um extrator de atributos. Esses atributos podem ser utilizados em outras tarefas como detecção de objetos, segmentação, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 14, 768])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = timm.create_model(tag, pretrained=True, num_classes=0, global_pool='')\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    scores = model(x)\n",
    "\n",
    "# Token para classificação\n",
    "cls_token = scores[:,0]\n",
    "# Mapa de atributos de tamanho 14x14\n",
    "feature_map = scores[:,1:].reshape(1,14,14,-1)\n",
    "feature_map.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos treinados no ImageNet\n",
    "\n",
    "Exemplos de modelos treinados no ImageNet. A técnica de treinamento é descrita no artigo \"How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers\" https://arxiv.org/abs/2106.10270\n",
    "\n",
    "* ImageNet 1k: 1281167 imagens separadas em 1000 classes\n",
    "* ImageNet 21k: 14197122 images separadas em 21841 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo treinado no ImageNet 1k com técnicas robustas de aumento de dados (aug)\n",
    "# e regularização (reg)\n",
    "tag = 'vit_base_patch16_224.augreg_in1k'\n",
    "# Modelo treinado no ImageNet 21k com augreg e refinado no ImageNet 1k\n",
    "tag = 'vit_base_patch16_224.augreg_in21k_ft_in1k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo treinado para recuperar regiões apagadas de imagens\n",
    "\n",
    "Uma técnica atual muito popular para realizar o pré-treinamento de modelos, chamada de *masked autoencoder* (MAE) consiste em apagar regiões de imagens e treinar um modelo para recuperar essas regiões. O processo é similar ao utilizado no modelo BERT em NLP. A técnica foi definida no artigo \"Masked Autoencoders Are Scalable Vision Learners\" https://arxiv.org/abs/2111.06377\n",
    "\n",
    "![](../data/notebook_images/mae.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinado com a técnica MAE no ImageNet 1k\n",
    "tag = 'vit_base_patch16_224.mae'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo treinado para medir a similaridade entre imagens transformadas\n",
    "\n",
    "Outra técnica muito popular consiste em treinar um modelo que aprende a similaridade entre imagens. Dado um batch de N imagens, cada imagem sofre duas operações de *data augmentation*. O modelo é treinado para que as saídas associadas à mesma imagem aumentada sejam mais iguais entre si do que a saída para outras imagens do dataset. Para isso, é definida uma função de loss apropriada.\n",
    "\n",
    "A técnica foi originalmente definida no artigo \"A Simple Framework for Contrastive Learning of Visual Representations\" (https://arxiv.org/abs/2002.05709). Um dos modelos mais usados atualmente é o DINO (\"Emerging Properties in Self-Supervised Vision Transformers\" https://arxiv.org/pdf/2104.14294).\n",
    "\n",
    "![](../data/notebook_images/SimCLR.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo treinado para medir a similaridade entre imagens em um dataset de 142 \n",
    "# milhões de imagens\n",
    "tag = 'vit_base_patch14_reg4_dinov2.lvd142m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos treinados para medir a similaridade entre textos e imagens\n",
    "\n",
    "Provavelmente a técnica mais popular atualmente é pré-treinar o modelo em pares de textos e imagens. O modelo aprende a medir a similaridade entre textos e imagens extraídos da web (Flickr, Wikipedia, ou qualquer outro site que possua textos e imagens). O modelo foi originalmente criado pela OpenAI \"Learning Transferable Visual Models From Natural Language Supervision\" https://arxiv.org/abs/2103.00020. O problema do modelo original é que o dataset utilizado para o treinamento não é conhecido. Por causa disso, foi criado o modelo OpenCLIP.\n",
    "\n",
    "* OpenAI: 400 milhões de pares de textos e imagens\n",
    "* OpenCLIP: 2.32 **bilhões** de pares de textos e imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo treinado pela OpenAI, refinado no ImageNet 12k (um dataset específico da\n",
    "# biblioteca timm baseado no ImageNet) e refinado no ImageNet 1k\n",
    "tag = 'vit_base_patch16_clip_224.openai_ft_in12k_in1k'\n",
    "# Modelo OpenCLIP\n",
    "tag = 'vit_base_patch16_clip_224.laion2b_ft_in12k_in1k'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo para extração de atributos em múltiplas escalas\n",
    "\n",
    "O modelo Swin Transformer possui algumas alterações em relação ao ViT baseadas em algumas características de CNNs (em particular, da ResNet). Ele permite extrair atributos de imagens em múltiplas escalas com alta resolução. Isso é possível porque a camada de atenção processa os tokens em janelas ao invés de usar uma atenção global. Modelo definido em \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\" (https://arxiv.org/abs/2103.14030)\n",
    "\n",
    "![](../data/notebook_images/swin.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 256, 64, 64])\n",
      "torch.Size([1, 512, 32, 32])\n",
      "torch.Size([1, 1024, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# Patches de tamanho 4x4. Atenção entre tokens é realizada em janelas de\n",
    "# tamanho 7x7 tokens\n",
    "tag = 'swin_base_patch4_window7_224'\n",
    "model = timm.create_model(tag, pretrained=True, img_size=512)\n",
    "model.eval()\n",
    "\n",
    "x = torch.rand(1, 3, 512, 512)\n",
    "with torch.no_grad():\n",
    "    scores, features = model.forward_intermediates(x)\n",
    "\n",
    "# Atributos extraídos pelo modelo de forma similar ao que implementamos usando\n",
    "# a ResNet\n",
    "for feat in features:\n",
    "    print(feat.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
