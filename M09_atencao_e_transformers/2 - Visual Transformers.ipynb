{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo ViT\n",
    "\n",
    "Implementação do Visual Transformer. Referências:\n",
    "\n",
    "* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "* [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)\n",
    "* https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurações iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class PatchifyLayer(nn.Module):\n",
    "    \"\"\"Módulo que transforma uma imagem em um conjunto de tokens. Mesmo\n",
    "    módulo implementado no notebook anterior.\"\"\"\n",
    "        \n",
    "    def __init__(self, image_size, patch_size, token_dim):\n",
    "        super().__init__()\n",
    "        self.conv_proj = nn.Conv2d(\n",
    "            3, token_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        new_size = image_size//patch_size\n",
    "        seq_length = new_size**2\n",
    "        self.token_dim = token_dim\n",
    "        self.new_size = new_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_proj(x)\n",
    "        x = x.reshape(x.shape[0], self.token_dim, -1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# Parâmetros de teste. Os nomes utilizados são os mesmos do código-fonte do Pytorch\n",
    "bs = 8             # batch size\n",
    "image_size = 224   # tamanho da imagem\n",
    "patch_size = 16    # tamanho dos patches 16x16 para gerar tokens\n",
    "num_layers = 12    # número de camadas\n",
    "num_heads = 12     # número de cabeças para a multihead attention\n",
    "token_dim = 768    # dimensão de cada token\n",
    "mlp_dim = 3072     # dimensão da camada linear após a atenção\n",
    "seq_length = (image_size//patch_size)**2  # tamanho de cada sequência"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camada linear\n",
    "\n",
    "A camada linear implementada abaixo será utilizada após cada camada de atenção. Ela é uma camada simples formada por linear->relu->linear. As camadas lineares incluem uma expansão de canais, ou seja, o número de canais é aumentado na primeira camada e reduzido na segunda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 196, 768])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Camada multilayer perceptron / feedforward. \n",
    "    Nota: Usualmente mlp_dim>token_dim.\"\"\"\n",
    "\n",
    "    def __init__(self, token_dim, mlp_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            torch.nn.Linear(token_dim, mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.Linear(mlp_dim, token_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "x = torch.rand(bs, seq_length, token_dim)\n",
    "mlp = MLP(token_dim, mlp_dim)\n",
    "out = mlp(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloco do codificador\n",
    "\n",
    "Um transformer consiste em uma sequência de blocos de codificação. Esses blocos são equivalentes ao ResidualBlock que implementamos para a ResNet (conv->batchnorm->relu->conv->batchnorm->relu), mas no caso do transformer temos layernorm->attention->layernorm->mlp\n",
    "\n",
    "A camada LayerNorm faz o mesmo papel do BatchNorm. Poderíamos ter usado BatchNorm, mas na prática LayerNorm tende a funcionar melhor com transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 196, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Bloco codificador de um transformer.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, token_dim, mlp_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Normalização e atenção\n",
    "        self.ln_1 = nn.LayerNorm(token_dim)\n",
    "        self.attention = nn.MultiheadAttention(token_dim, num_heads, batch_first=True)\n",
    "\n",
    "        # Normalização e camada linear\n",
    "        self.ln_2 = nn.LayerNorm(token_dim)\n",
    "        self.mlp = MLP(token_dim, mlp_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = self.ln_1(input)\n",
    "        x, _ = self.attention(x, x, x)\n",
    "        # Adiciona resíduo (assim como na resnet)\n",
    "        x = x + input   \n",
    "        \n",
    "        y = self.ln_2(x)\n",
    "        y = self.mlp(y)\n",
    "\n",
    "        # Adciona resíduo e retorna\n",
    "        return x + y\n",
    "    \n",
    "x = torch.rand(bs, seq_length, token_dim)\n",
    "eb = EncoderBlock(num_heads, token_dim, mlp_dim)\n",
    "out = eb(x)\n",
    "out.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alguns conceitos extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token de classe\n",
    "\n",
    "O codificador do transformer realiza a mistura entre os tokens da sequência de entrada. Mas para aplicarmos o modelo em tarefas de classificação, precisamos de alguma forma extrair atributos da sequência como um todo, pois queremos caracterizar a imagem, e não os tokens. Por exemplo, para uma sequência de tamanho 1 x 196 x 768 representando uma imagem, queremos extrair um tensor de tamanho 1 x 768 contendo 768 atributos para toda a imagem. \n",
    "\n",
    "Uma forma de fazer isso seria simplesmente calcular a média dos valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imagem de exemplo\n",
    "x = torch.rand(bs, seq_length, token_dim)\n",
    "features = x.mean(dim=1)\n",
    "# 768 atributos para cada imagem\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas ao invés de obter a média é comum ser feito um outro procedimento. Um token especial responsável por caracterizar a imagem como um todo é adicionado à sequência. Esse token é um parâmetro *treinável* do modelo, ou seja, ele é modificado durante o treinamento para reduzir a loss function. **Apenas o valor desse token é usado em tarefas de classificação, o restante da sequência é descartado na saída do transformer**. Esse token especial será \"misturado\" com os outros tokens ao longo do transformer (por causa da atenção) e também otimizado para resultar na melhor classificação possível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token especial de classe\n",
    "class_token = nn.Parameter(torch.zeros(1, 1, token_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro do método .forward() do transformer teremos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs imagens representadas por seq_length tokens de dimensão token_dim\n",
    "x = torch.rand(bs, seq_length, token_dim)\n",
    "\n",
    "# Expansão do token de classe de 1 x 1 x token_dim -> bs x 1 x token_dim\n",
    "# Os valores são copiados para serem iguais ao longo do batch\n",
    "batch_class_token = class_token.expand(bs, -1, -1)\n",
    "# Concatena na dimensão da sequência\n",
    "x = torch.cat([batch_class_token, x], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional embedding\n",
    "\n",
    "Uma camada de atenção é equivariante à permutação da sequência de entrada. Isso quer dizer que se reordenarmos a sequência de entrada, o resultado da camada também será reordenado da mesma forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(bs, seq_length, token_dim)\n",
    "# Reordena os tokens da sequência de forma aleatória\n",
    "inds = torch.randperm(seq_length)\n",
    "x_p = x[:, inds]\n",
    "# Aplica a camada de atenção nas duas versões da sequência\n",
    "mha = nn.MultiheadAttention(num_heads=num_heads, embed_dim=token_dim)\n",
    "out = mha(x, x, x)[0]\n",
    "out_p = mha(x_p, x_p, x_p)[0]\n",
    "# Compara o resultado original permutado com o resultado usando a entrada permutada\n",
    "torch.allclose(out[:, inds], out_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso, a rede não terá qualquer informação sobre a posição original dos tokens na imagem. Por exemplo, o resultado para um patch no canto esquerdo inferior da imagem será o mesmo se o patch for inserido no centro da imagem. Em geral, é importante a rede ter informação sobre a posição do patch na imagem, de forma que essa informação possa ser usada no mecanismo de atenção. Para adiciona tal informação, usamos o chamado positional embedding, que é um tensor que possuirá informação sobre a posição de cada token na imagem.\n",
    "\n",
    "Um possível tensor com essa informação pode ser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embedding = torch.linspace(0, 1, seq_length*token_dim).reshape(seq_length, token_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse caso, para cada token `pos_embedding` possui um valor fixo que caracteriza unicamente cada token. Por exemplo, o token 10 terá o código pos_embedding[10] associado a ele. Tendo esse código, a prática comum é simplesmente somar o código aos tokens sendo processados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequência\n",
    "x = torch.rand(bs, seq_length, token_dim)\n",
    "# Adição da posição dos tokens\n",
    "x = x + pos_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ideia é que a rede poderá utilizar esse valor que foi somado como referência sobre a posição do token. Transformers podem ser treinados sem positional encoding, e em geral dão bons resultados. Mas a adição de posição tende a levar a uma pequena melhora na performance. \n",
    "\n",
    "Existem várias outras possíveis codificações para posição dos tokens. Curiosamente, atualmente é prática comum simplesmente utilizar um parâmetro treinável como positional encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos uma distribuição normal ao invés de torch.zeros para que os\n",
    "# valores inciais não sejam todos iguais. Essa estratégia foi usada no\n",
    "# artigo do modelo BERT\n",
    "pos_embedding = torch.normal(mean=0., std=0.02, size=(1, seq_length, token_dim))\n",
    "pos_embedding = nn.Parameter(pos_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "\n",
    "Finalmente podemos o modelo transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, image_size, patch_size, num_layers, num_heads, token_dim,\n",
    "        mlp_dim, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Transforma imagem em tokens\n",
    "        self.patchify = PatchifyLayer(image_size, patch_size, token_dim)\n",
    "        # Tamanho da sequência\n",
    "        seq_length = (image_size//patch_size)**2\n",
    "\n",
    "        # Adiciona token para a classe\n",
    "        self.class_token = nn.Parameter(torch.zeros(1, 1, token_dim))\n",
    "        seq_length += 1\n",
    "\n",
    "        # Informação sobre a posição\n",
    "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, token_dim).normal_(std=0.02))  \n",
    "\n",
    "        # Codificador\n",
    "        encoder = []\n",
    "        for _ in range(num_layers):\n",
    "            encoder.append(EncoderBlock(num_heads, token_dim, mlp_dim))\n",
    "        self.encoder = nn.Sequential(*encoder)\n",
    "        self.ln = nn.LayerNorm(token_dim)\n",
    "\n",
    "        # Camada de classificação\n",
    "        self.final = nn.Linear(token_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # bs x c x H x W -> bs x seq_length x token_dim\n",
    "        x = self.patchify(x)\n",
    "        bs = x.shape[0]\n",
    "\n",
    "        # Expansão do token de classe de 1 x 1 x token_dim -> bs x 1 x token_dim\n",
    "        batch_class_token = self.class_token.expand(bs, -1, -1)\n",
    "        # Concatena na dimensão da sequência\n",
    "        x = torch.cat([batch_class_token, x], dim=1)\n",
    "\n",
    "        # Adiciona embedding posicional\n",
    "        x = x + self.pos_embedding\n",
    "        x = self.ln(self.encoder(x))\n",
    "\n",
    "        # Extrai apenas o token de classe de cada batch\n",
    "        x = x[:, 0]\n",
    "\n",
    "        # Usa o token de classe para classificar a imagem\n",
    "        # bs x token_dim -> bs x num_classes\n",
    "        x = self.final(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "vit = VisionTransformer(image_size, patch_size, num_layers, num_heads, \n",
    "                        token_dim, mlp_dim, num_classes=1000)\n",
    "\n",
    "x = torch.rand(bs, 3, image_size, image_size)\n",
    "out = vit(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo do Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.image_size=224\n",
      "model.patch_size=16\n",
      "num_layers=12\n",
      "num_heads=12\n",
      "model.hidden_dim=768\n",
      "model.mlp_dim=3072\n",
      "model.seq_length=197\n",
      "model.num_classes=1000\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vision_transformer\n",
    "\n",
    "model = vision_transformer.vit_b_16()\n",
    "\n",
    "print(f'{model.image_size=}')\n",
    "print(f'{model.patch_size=}')\n",
    "print(f'num_layers={len(model.encoder.layers)}')\n",
    "print(f'num_heads={model.encoder.layers[0].num_heads}')\n",
    "print(f'{model.hidden_dim=}')\n",
    "print(f'{model.mlp_dim=}')\n",
    "print(f'{model.seq_length=}')\n",
    "print(f'{model.num_classes=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se desejarmos utilizar o transformer para outra tarefa além de classificação, podemos extrair os atributos gerados pelo modelo do Pytorch assim como fizemos para ResNets. Basta fazermos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.heads = nn.Identity()\n",
    "out = model(x)\n",
    "# 768 atributos extraídos de cada imagem do batch\n",
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
