{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mecanismos de atenção\n",
    "\n",
    "Atenção consiste em combinar diferentes informações sendo processadas por uma rede neural de forma a aumentar o contexto dos atributos. Por exemplo, ao processar uma imagem, filtros de CNNs combinam valores bem locais (ex: regiões 3x3), e são necessárias muitas camadas para que a rede consiga combinar regiões distintias de uma imagem. Mecanismos de atenção podem ser utilizados para combinar regiões distantes de uma imagem, o que permite que cada região seja processada com um contexto global."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atenção de canais\n",
    "\n",
    "A atenção de canais envolve enaltecer canais de ativação que são importantes e suprimir canais com menor relevância. Um módulo muito utilizado é o chamado *squeeze-and-excitation*, que facilita a modulação dos valores dos canais coonforme a necessidade de processamento da rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16, 64, 64])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, squeeze_channels=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): nro de canais de entrada\n",
    "            squeeze_channels (int): nro de canais intermediários\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if squeeze_channels is None:\n",
    "            squeeze_channels = in_channels\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_channels, squeeze_channels, 1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(squeeze_channels, in_channels, 1)\n",
    "        self.scale_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # bs x C x 1 x 1\n",
    "        scale = self.avgpool(x)\n",
    "        # Mistura os canais com uma convolução 1 x 1 bottleneck, que reduz\n",
    "        # o número de canais\n",
    "        scale = self.fc1(scale)\n",
    "        scale = self.activation(scale)\n",
    "        # Aumenta novamente o número de canais\n",
    "        scale = self.fc2(scale)\n",
    "        # Normaliza no intervalo [0,1]\n",
    "        scale = self.scale_activation(scale)\n",
    "        # Reescala os canais \n",
    "        # bs x C x 1 x 1 * bs x C x H x W\n",
    "        x = scale*x\n",
    "\n",
    "        return x\n",
    "    \n",
    "se = SqueezeExcitation(16, 4)\n",
    "\n",
    "# Batch com 8 ativações, cada uma possuindo 16 canais e tamanho 64x64\n",
    "y = se(torch.rand(8, 16, 64, 64))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atenção espacial\n",
    "\n",
    "A atenção espacial consiste em transformar regiões da imagem em tokens. Esses tokens não possuem uma posição espacial, eles são tratados como um *bag of tokens* que podem ser combinados de forma independente, sem levar em conta a proximidade entre eles. Tokens são exatamente o mesmo conceito encontrado em processamento de língua natural (NLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 196, 768])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PatchifyLayer(nn.Module):\n",
    "    \"\"\"Módulo que transforma uma imagem em um conjunto de tokens.\"\"\"\n",
    "        \n",
    "    def __init__(self, image_size, patch_size, token_dim):\n",
    "        \"\"\"`image_size` precisa ser divisível por `patch_size`.\n",
    "\n",
    "        Args:\n",
    "            image_size (int): tamanho da imagem que será processada.\n",
    "            patch_size (int): tamanho das regiões que serão transformada em tokens.\n",
    "            token_dim (int): número de atributos gerados para cada token.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Note o stride. Essa camada transforma cada região patch_size x patch_size \n",
    "        # da imagem em token_dim x 1 x 1\n",
    "        self.conv_proj = nn.Conv2d(\n",
    "            3, token_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "        # Novo tamanho da imagem\n",
    "        new_size = image_size//patch_size\n",
    "        # Tamanho da sequência de tokens\n",
    "        seq_length = new_size**2\n",
    "\n",
    "        self.token_dim = token_dim\n",
    "        self.new_size = new_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # (bs, c, image_size, image_size) -> (bs, token_dim, new_size, new_size)\n",
    "        x = self.conv_proj(x)\n",
    "        # (bs, token_dim, new_size, new_size) -> (bs, token_dim, (new_size*new_size))\n",
    "        x = x.reshape(x.shape[0], self.token_dim, -1)\n",
    "        # Coloca a dimensão espacial como segunda, pois o padrão de camadas de \n",
    "        # atenção é bs x seq_length x token_dim\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 8 imagens RGB de tamanho 224 x 224\n",
    "x = torch.rand(8, 3, 224, 224)\n",
    "pl = PatchifyLayer(image_size=224, patch_size=16, token_dim=768)\n",
    "tokens = pl(x)\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada imagem em um batch é representada por 196 tokens, cada um possuindo 768 atributos\n",
    "\n",
    "Tendo a imagem representada como uma sequência, podemos aplicar um mecanismo de atenção à imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 196, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def attention(query, key, value):\n",
    "    \n",
    "    # Tamanho de cada token\n",
    "    d_k = query.shape[-1]\n",
    "    # Similaridade entre cada par de tokens da sequência\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / d_k**0.5\n",
    "    # Normaliza a similaridade entre [0,1]\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    # Atualiza os valores dos tokens de acordo com as similaridades\n",
    "    value = torch.matmul(p_attn, value)\n",
    "\n",
    "    return value\n",
    "\n",
    "out = attention(tokens, tokens, tokens)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que o mecanismo de atenção não possui parâmetros treináveis!\n",
    "\n",
    "O artigo original sobre atenção define a chamada multi-headed attention, que consiste em realizar diversas atenções em paralelo para aumentar o poder de expressividade do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 196, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, heads, token_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Valor usado para normalização\n",
    "        d_k = token_dim//heads\n",
    "        self.heads = heads\n",
    "        self.d_k = d_k\n",
    "        # Camadas de projeção antes da atenção\n",
    "        self.proj_query = nn.Linear(token_dim, token_dim)\n",
    "        self.proj_key = nn.Linear(token_dim, token_dim)\n",
    "        self.proj_value = nn.Linear(token_dim, token_dim)\n",
    "        self.final = nn.Linear(token_dim, token_dim)\n",
    "\n",
    "    def proj_and_reshape(self, layer, x):\n",
    "        '''Aplica uma transformação linear e redimensiona o resultado\n",
    "        para ser usado na função de atenção.'''\n",
    "\n",
    "        bs = x.shape[0]\n",
    "        # A multiplicação x*layer.weight abaixo possui dimensão:\n",
    "        # (bs x n x token_dim) * (token_dim x heads*d_k)\n",
    "        # onde n é o tamanho da sequência.\n",
    "        # Cada sequência com token_dim (heads*d_k) atributos é multiplicada por \n",
    "        # uma coluna da camada linear. Isso é equivalente a fazer a sequinte operação:\n",
    "        # Aplicar `heads`` camadas, cada uma com tamanho token_dim x d_k, nas sequências\n",
    "        # e depois concatenar os resultados. \n",
    "        x = layer(x)\n",
    "        # Visualiza o resultado como uma matriz bs x heads x n x d_k. Isso\n",
    "        # possibilita aplicar a função `attention` nas dimensões n x d_k\n",
    "        x = x.view(bs, -1, self.heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "\n",
    "        nbatches = query.shape[0]\n",
    "        query_proj = self.proj_and_reshape(self.proj_query, query)\n",
    "        key_proj = self.proj_and_reshape(self.proj_key, key)\n",
    "        value_proj = self.proj_and_reshape(self.proj_value, value)\n",
    "\n",
    "        x = attention(query_proj, key_proj, value_proj)\n",
    "        # Redimensiona de bs x heads x n x d_k para bs x n x token_dim\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.heads*self.d_k)\n",
    "\n",
    "        return self.final(x)\n",
    "    \n",
    "mha = MultiHeadedAttention(heads=12, token_dim=768)\n",
    "out = mha(tokens, tokens, tokens)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Pytorch possui uma camada que faz exatamente o que implementamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 196, 768]) torch.Size([8, 196, 196])\n"
     ]
    }
   ],
   "source": [
    "mha = nn.MultiheadAttention(embed_dim=768, num_heads=12, batch_first=True)\n",
    "out, attn_weights  = mha(tokens, tokens, tokens)\n",
    "print(out.shape, attn_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A camada também retorna a multiplicação entre as chaves e queries (variável `p_attn` na nossa função de atenção). Essa variável é útil para verificar o relacionamento entre os tokens da sequência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2304, 768])\n",
      "torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "print(mha.in_proj_weight.shape)\n",
    "print(mha.out_proj.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O atributo `in_proj_weight` é a matriz de pesos das projeções das chaves, queries e valores. O Pytorch realiza as projeções em uma única multiplicação matricial. Para fazer isso basta concatenar as matrizes `proj_query`, `proj_key` e `proj_value` da nossa classe. Não fizemos dessa forma para deixar o código mais simples de entender.\n",
    "\n",
    "O atributo `out_proj` é a camada linear de saída que implementamos (`final`)\n",
    "\n",
    "Até o momento utilizamos a chamada *self-attention*, que consiste em utilizar a mesma variável como query, key e value. Mas o mecanismo de atenção pode ser utilizado de forma natural para misturar diferentes informações. Por exemplo, uma camada de atenção pode receber como entrada atributos sobre um texto e sobre uma imagem. Nesse caso, é comum associar `key` e `value` com os atributos do texto e `query` com os atributos da imagem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 196, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch com 8 sequências de tokens de imagens\n",
    "tokens_img = torch.rand(8, 196, 768)\n",
    "# batch com 8 sequências de tokens de texto, cada texto possui 20 tokens e\n",
    "# cada token 512 atributos. Por exemplo, esses textos podem ser descrições\n",
    "# da imagem como \"Uma imagem de um cachorro dormindo\"\n",
    "tokens_text = torch.rand(8, 20, 768)\n",
    "\n",
    "#              query         key         value\n",
    "out, _ = mha(tokens_img, tokens_text, tokens_text)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os nomes *key*, *value* e *query* referenciam conceitos de banco de dados. Podemos considerar que *key* e *value* representam as chaves e valores de itens de um banco de dados. *query* é uma busca feita no banco. A similaridade entre a busca (*query*) e as chaves (*keys*) é calculada. A busca vai ser mais similar a algumas chaves do que outras. Os elementos mais similares encontrados na busca são usados para atualizar os valores de *value*, e esses valores atualizados são a saída da camada. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
