{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento na GPU [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/chcomin/curso-visao-computacional-2024/blob/main/M06_classificacao_de_imagens_naturais/3%20-%20Processamento%20na%20GPU%20\\(GPU\\).ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copiando dados entre CPU e GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "\n",
    "x = torch.rand(16,3,224,224)\n",
    "model = models.resnet18()\n",
    "print(x.device)\n",
    "print(model.conv1.weight.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Move o tensor para a GPU\n",
    "x_cuda = x.to('cuda')\n",
    "# Move todos os tensores de parâmetros do modelo para a GPU. Note que como `modelo`\n",
    "# é um objeto, os atributos dele é que são modificados. A referência para o modelo na\n",
    "# CPU é perdida\n",
    "model.to('cuda')\n",
    "\n",
    "print(x.device)\n",
    "print(x_cuda.device)\n",
    "print(model.conv1.weight.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os dados e o modelo na GPU, é possível aplicar o modelo normalmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portanto, um loop de treinamento padrão é feito da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'  # Pode ser 'cuda' ou 'cpu'\n",
    "\n",
    "# Dataloader artificial só para ilustração\n",
    "target = torch.zeros(16, dtype=torch.long)\n",
    "dl = [(x,target)]*10\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_log = 0.\n",
    "for imgs, targets in dl:\n",
    "    imgs = imgs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    model.zero_grad()\n",
    "    scores = model(imgs)\n",
    "    loss = loss_func(scores, targets)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    # Como já vimos, é útil armazenar os valores da loss para plotar. Por isso\n",
    "    # aplicamos um detach() para remoção do grafo de computação. Mas é importante\n",
    "    # tomar cuidado porque que o valor ainda está na GPU\n",
    "    loss_log += loss.detach()\n",
    "# Podemos copiar o valor de volta para a CPU após processar todos os batches\n",
    "loss_log = loss_log.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programação assíncrona\n",
    "\n",
    "É importante notar que a execução na CPU e na GPU é feita de forma assíncrona, isto é, enquanto a GPU está processando os dados, o programa continua executando na CPU. Vamos entender isso na prática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011183200000232318\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# Matriz de tamanho 4000 x 4000\n",
    "x = torch.randn(4000, 4000, device='cuda')\n",
    "\n",
    "ti = time.perf_counter()\n",
    "for i in range(500):\n",
    "    # Multiplicação matricial\n",
    "    y = torch.matmul(x, x)\n",
    "tt = time.perf_counter() - ti\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código acima executa extremamente rápido! \n",
    "\n",
    "...será? Veja o que acontece se repetirmos exatamente o mesmo código mas mandarmos imprimir um valor do resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.0184, device='cuda:0')\n",
      "0.014413899996725377 4.061334799996985\n"
     ]
    }
   ],
   "source": [
    "ti = time.perf_counter()\n",
    "for i in range(500):\n",
    "    y = torch.matmul(x, x)\n",
    "# Tempo de execução do loop\n",
    "t_loop = time.perf_counter() - ti\n",
    "print(y[0,0])\n",
    "# Tempo de execução do loop + print\n",
    "t_print = time.perf_counter() - ti\n",
    "print(t_loop, t_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O tempo medido após o print é muito maior do que o medido logo antes do print! Por acaso o print demorou para executar? Não, o que aconteceu é que a impressão de um valor de `y` é uma tarefa que bloqueia a CPU. O processo fica esperando a GPU terminar os cálculos para poder imprimir o valor.\n",
    "\n",
    "O fato da execução na CPU e GPU serem assíncronas possui duas implicações importantes:\n",
    "\n",
    "* Para garantir a máxima performance, é preciso tomar cuidado com operações que bloqueiam a CPU. Cópias entre a CPU e GPU bloqueiam, assim como impressão de valores na tela.\n",
    "* É preciso tomar cuidado ao medir o tempo de execução de um código. O tempo é medido no processo da CPU, e não necessariamente leva em conta o tempo de execução na GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiar o resultado da GPU para a CPU bloqueia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.823698799998965\n"
     ]
    }
   ],
   "source": [
    "ti = time.perf_counter()\n",
    "for i in range(500):\n",
    "    y = torch.matmul(x, x)\n",
    "    loss = y.sum()\n",
    "    loss = loss.to('cpu')\n",
    "    # O mesmo ocorreria com os comandos\n",
    "    #loss = loss.item()\n",
    "    #print(loss)\n",
    "tt = time.perf_counter() - ti\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medindo performance\n",
    "\n",
    "Vamos ver técnicas simples para medir o tempo de execução na CPU e GPU e o uso de memória na GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011s, 3.745s, 0.4GiB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "def proc():\n",
    "    '''Processamento a ser executado'''\n",
    "    x = torch.randn(4000, 4000, device='cuda')\n",
    "    for i in range(500):\n",
    "        y = torch.matmul(x, x)\n",
    "\n",
    "def benchmark(func):\n",
    "\n",
    "    # Eventos de medida de tempo na GPU\n",
    "    gpu_start = torch.cuda.Event(enable_timing=True)\n",
    "    gpu_end = torch.cuda.Event(enable_timing=True)  \n",
    "    # Apaga registro de pico de memória\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # Tempo inicial na CPU\n",
    "    ti = time.perf_counter()\n",
    "    # Envia um comando para a GPU para registrar o tempo\n",
    "    gpu_start.record() \n",
    "    func()\n",
    "    # Tempo final na CPU\n",
    "    t_cpu = time.perf_counter() - ti\n",
    "    # Tempo final na GPU\n",
    "    gpu_end.record()\n",
    "    # Bloqueia a CPU para esperar a GPU terminar os cálculos\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t_gpu = gpu_start.elapsed_time(gpu_end)/1000\n",
    "    max_memory = torch.cuda.max_memory_allocated()/2**30\n",
    "\n",
    "    return t_cpu, t_gpu, max_memory\n",
    "\n",
    "t1, t2, m = benchmark(proc)\n",
    "print(f'{t1:.3f}s, {t2:.3f}s, {m:.1f}GiB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mantendo a GPU ocupada (throughput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acima medimos o tempo de execução, mas também é importante verificarmos se a GPU está sendo usada de forma efetiva. Uma GPU possui milhares de cores, que estão organizados nos chamados *streaming multiprocessors (SM)*. Por exemplo, uma RTX 3080 12 GB possui 70 SMs, e cada SM possui:\n",
    "1. 128 cores para operações de ponto flutuante de 32 bits (FP32);\n",
    "2. 2 cores para operações de ponto flutuante de 64 bits;\n",
    "3. 64 cores para operações com inteiro 32 bits;\n",
    "4. 4 tensor cores\n",
    "\n",
    "Dependendo da operação, cores distintos serão usados pelo SM. Em operações FP32, a RTX 3080 12 GB possui 128*70 = 8960 cores FP32. Portanto, a cada ciclo de clock podem ser realizadas 8960 operações em paralelo. Essa GPU trabalha em uma frequência máxima de 1710 MHz. Portanto, ela consegue realizar 15.32 TFLOPS operações por segundo. Esse é chamado de *throughput* máximo. A situação ideal é termos 100% de uso dos cores a todo momento. Uma forma de verificar o uso da GPU é através do comando `nvidia-smi dmon` no terminal. Mas esse comando pode dar resultados imprecisos. O nvidia-smi em geral mede a cada segundo a fração de tempo que **ao menos um SM esteve ativo**. Por exemplo, 100% de uso da GPU pode significar:\n",
    "\n",
    "1. A GPU utilizou 100% dos SMs no período\n",
    "2. A GPU utilizou apenas 1 SM no período\n",
    "\n",
    "Para ilustrar a situação, executando o seguinte código em uma RTX 3080:\n",
    "\n",
    "```python\n",
    "    x = torch.tensor(0., device='cuda')\n",
    "    for i in range(100000):\n",
    "        x = x + 1.\n",
    "```\n",
    "o nvidia-smi mostrou 35% de uso da GPU. Mas esse código é completamente sequencial. A cada instante apenas um único core dentre os 8960 realiza uma operação. Então a GPU ficou praticamente sem uso."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
