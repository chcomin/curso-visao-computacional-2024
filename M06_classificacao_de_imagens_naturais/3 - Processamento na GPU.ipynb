{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento na GPU [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/chcomin/curso-visao-computacional-2024/blob/main/M06_classificacao_de_imagens_naturais/3%20-%20Processamento%20na%20GPU.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copiando dados entre CPU e GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "\n",
    "x = torch.rand(16,3,224,224)\n",
    "model = models.resnet18()\n",
    "print(x.device)\n",
    "print(model.conv1.weight.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Move o tensor para a GPU\n",
    "x_cuda = x.to('cuda')\n",
    "# Move todos os tensores de parâmetros do modelo para a GPU. Note que como `modelo`\n",
    "# é um objeto, os atributos dele é que são modificados. A referência para o modelo na\n",
    "# CPU é perdida\n",
    "model.to('cuda')\n",
    "\n",
    "print(x.device)\n",
    "print(x_cuda.device)\n",
    "print(model.conv1.weight.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os dados e o modelo na GPU, é possível aplicar o modelo normalmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portanto, um loop de treinamento padrão é feito da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'  # Pode ser 'cuda' ou 'cpu'\n",
    "\n",
    "# Dataloader artificial só para ilustração\n",
    "target = torch.zeros(16, dtype=torch.long)\n",
    "dl = [(x,target)]*10\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "for imgs, targets in dl:\n",
    "    imgs = imgs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    model.zero_grad()\n",
    "    scores = model(imgs)\n",
    "    loss = loss_func(scores, targets)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    # Para logar a loss, podemos usar .item(), o que copia o valor de volta para a CPU\n",
    "    loss_log = loss.item()\n",
    "    # Podemos também usar .detach(), o que mantém o valor na GPU e evita uma cópia\n",
    "    #loss_log = loss.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programação assíncrona\n",
    "\n",
    "É importante notar que a execução na CPU e na GPU é feita de forma assíncrona, isto é, enquanto a GPU está processando os dados, o programa continua executando na CPU. Vamos entender isso na prática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026443400000061956\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# 100 matrizes de tamanho 900 x 900\n",
    "x = torch.randn(100, 900, 900, device='cuda')\n",
    "\n",
    "ti = time.perf_counter()\n",
    "for i in range(500):\n",
    "    # Multiplicação de 100 matrizes\n",
    "    y = torch.matmul(x, x)\n",
    "tt = time.perf_counter() - ti\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código acima executa extremamente rápido! \n",
    "\n",
    "...será? Veja o que acontece se repetirmos exatamente o mesmo código mas mandarmos imprimir um valor do resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.5057, device='cuda:0')\n",
      "0.023584899999150366 10.395847499999945\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "x = torch.randn(100, 900, 900, device='cuda')\n",
    "\n",
    "ti = time.perf_counter()\n",
    "for i in range(500):\n",
    "    y = torch.matmul(x, x)\n",
    "# Tempo de execução do loop\n",
    "t_loop = time.perf_counter() - ti\n",
    "print(y[0,0,0])\n",
    "# Tempo de execução do loop + print\n",
    "t_print = time.perf_counter() - ti\n",
    "print(t_loop, t_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O tempo medido após o print é muito maior do que o medido logo antes do print! Por acaso o print demorou para executar? Não, o que aconteceu é que a impressão de um valor de `y` é uma tarefa que bloqueia a CPU. O processo fica esperando a GPU terminar os cálculos para poder imprimir o valor.\n",
    "\n",
    "O fato da execução na CPU e GPU serem assíncronas possui duas implicações importantes:\n",
    "\n",
    "* Para garantir a máxima performance, é preciso tomar cuidado com operações que bloqueiam a CPU. Cópias entre a CPU e GPU bloqueiam, assim como impressão de valores na tela.\n",
    "* É preciso tomar cuidado ao medir o tempo de execução de um código. O tempo é medido no processo da CPU, e não necessariamente leva em conta o tempo de execução na GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiar o resultado da GPU para a CPU bloqueia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.440150800000083\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(100, 900, 900, device='cuda')\n",
    "\n",
    "ti = time.perf_counter()\n",
    "for i in range(500):\n",
    "    y = torch.matmul(x, x)\n",
    "    loss = y.sum()\n",
    "    loss = loss.to('cpu')\n",
    "    # O mesmo ocorreria com os comandos\n",
    "    #loss = loss.item()\n",
    "    #print(loss)\n",
    "tt = time.perf_counter() - ti\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medindo performance\n",
    "\n",
    "Vamos ver técnicas simples para medir o tempo de execução na CPU e GPU e o uso de memória na GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017s, 5.505s, 1.6GiB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "def proc():\n",
    "    '''Processamento a ser executado'''\n",
    "    x = torch.randn(100, 900, 900, device='cuda')\n",
    "    for i in range(500):\n",
    "        y = torch.matmul(x, x)\n",
    "\n",
    "def benchmark(func):\n",
    "\n",
    "    # Eventos de medida de tempo na GPU\n",
    "    gpu_start = torch.cuda.Event(enable_timing=True)\n",
    "    gpu_end = torch.cuda.Event(enable_timing=True)  \n",
    "    # Apaga registro de pico de memória\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # Tempo inicial na CPU\n",
    "    ti = time.perf_counter()\n",
    "    # Envia um comando para a GPU para registrar o tempo\n",
    "    gpu_start.record() \n",
    "    func()\n",
    "    # Tempo final na CPU\n",
    "    t_cpu = time.perf_counter() - ti\n",
    "    # Tempo final na GPU\n",
    "    gpu_end.record()\n",
    "    # Espera a GPU terminar os cálculos\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t_gpu = gpu_start.elapsed_time(gpu_end)/1000\n",
    "    max_memory = torch.cuda.max_memory_allocated()/2**30\n",
    "\n",
    "    return t_cpu, t_gpu, max_memory\n",
    "\n",
    "t1, t2, m = benchmark(proc)\n",
    "print(f'{t1:.3f}s, {t2:.3f}s, {m:.1f}GiB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
