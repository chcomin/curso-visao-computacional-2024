{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atenção de canais\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16, 64, 64])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, squeeze_channels=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if squeeze_channels is None:\n",
    "            squeeze_channels = in_channels\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_channels, squeeze_channels, 1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(squeeze_channels, in_channels, 1)\n",
    "        self.scale_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # bs x C x 1 x 1\n",
    "        scale = self.avgpool(x)\n",
    "        # Mistura os canais\n",
    "        scale = self.fc1(scale)\n",
    "        scale = self.activation(scale)\n",
    "        scale = self.fc2(scale)\n",
    "        # Normaliza no intervalo [0,1]\n",
    "        scale = self.scale_activation(scale)\n",
    "        # Reescala os canais \n",
    "        x = scale*x\n",
    "\n",
    "        return x\n",
    "    \n",
    "se = SqueezeExcitation(16, 4)\n",
    "y = se(torch.rand(8, 16, 64, 64))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atenção espacial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 196, 768])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PatchifyLayer(nn.Module):\n",
    "        \n",
    "    def __init__(self, image_size=224, patch_size=16, hidden_dim=768):\n",
    "        '''`image_size` precisa ser divisível por `patch_size`.'''\n",
    "        super().__init__()\n",
    "\n",
    "        # Cada região patch_size x patch_size da imagem é transformada \n",
    "        # em 1 x 1\n",
    "        self.conv_proj = nn.Conv2d(\n",
    "            3, hidden_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "        new_size = image_size//patch_size\n",
    "        seq_length = new_size**2\n",
    "\n",
    "        self.new_size = new_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # (bs, c, image_size, image_size) -> (bs, hidden_dim, new_size, new_size)\n",
    "        x = self.conv_proj(x)\n",
    "        # (n, hidden_dim, new_size, new_size) -> (n, hidden_dim, (new_size*new_size))\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "        # Coloca a dimensão espacial como segunda, pois o padrão de camadas de \n",
    "        # atenção é bs x seq_length x hidden_dim\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "x = torch.rand(8, 3, 224, 224)\n",
    "pl = PatchifyLayer()\n",
    "y = pl(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada imagem em um batch é representada por 196 tokens, cada um possuindo 768 atributos\n",
    "\n",
    "Artigo transformers: d_model = 512, heads=8\n",
    "ViT: d_model/embed_dim = 768, heads=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 196, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def attention(query, key, value):\n",
    "    \n",
    "    d_k = query.shape[-1]\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / d_k**0.5\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    out = torch.matmul(p_attn, value)\n",
    "\n",
    "    return out\n",
    "\n",
    "x = torch.rand(16, 196, 768)\n",
    "out = attention(x, x, x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 196, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, heads=12, d_model=768):\n",
    "        super().__init__()\n",
    "\n",
    "        # Assumimos d_k=d_v\n",
    "        d_k = d_model//heads\n",
    "        self.heads = heads\n",
    "        self.d_k = d_k\n",
    "        self.proj_query = nn.Linear(d_model, d_model)\n",
    "        self.proj_key = nn.Linear(d_model, d_model)\n",
    "        self.proj_value = nn.Linear(d_model, d_model)\n",
    "        self.final = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def proj_and_reshape(self, layer, x):\n",
    "        '''Aplica uma transformação linear e redimensiona o resultado\n",
    "        para ser usado na função de atenção.'''\n",
    "\n",
    "        nbatches = x.shape[0]\n",
    "        # A multiplicação x*layer.weight abaixo possui dimensão:\n",
    "        # bs x n x d_model * d_model x heads*d_k\n",
    "        # Cada sequência de tamanho d_model é multiplicada por uma coluna da \n",
    "        # camada linear. Isso é equivalente a fazer a sequinte operação:\n",
    "        # Aplicar `heads`` camadas, cada uma com tamanho d_model x d_k, nas sequências\n",
    "        # e depois concatenar. \n",
    "        x = layer(x)\n",
    "        # Visualiza o resultado como uma matriz bs x heads x n x d_k. Isso\n",
    "        # possibilita aplicar a função `attention` nas dimensões n x d_k\n",
    "        x = x.view(nbatches, -1, self.heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "\n",
    "        nbatches = query.shape[0]\n",
    "        query_proj = self.proj_and_reshape(self.proj_query, query)\n",
    "        key_proj = self.proj_and_reshape(self.proj_key, key)\n",
    "        value_proj = self.proj_and_reshape(self.proj_value, value)\n",
    "\n",
    "        x = attention(query_proj, key_proj, value_proj)\n",
    "        # Redimensiona de bs x heads x n x d_k para bs x n x d_model\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.heads*self.d_k)\n",
    "\n",
    "        return self.final(x)\n",
    "    \n",
    "x = torch.rand(8, 196, 768)\n",
    "mha = MultiHeadedAttention()\n",
    "y = mha(x, x, x)\n",
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = nn.MultiheadAttention(embed_dim=768, num_heads=12, batch_first=True)\n",
    "y = mha(x, x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=768, hidden_channels=3072):\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            torch.nn.Linear(in_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_channels, in_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Attention block\n",
    "        self.ln_1 = norm_layer(hidden_dim)\n",
    "        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=attention_dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # MLP block\n",
    "        self.ln_2 = norm_layer(hidden_dim)\n",
    "        self.mlp = MLP(hidden_dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        x = self.ln_1(input)\n",
    "        x, _ = self.self_attention(x, x, x, need_weights=False)\n",
    "        x = self.dropout(x)\n",
    "        x = x + input\n",
    "\n",
    "        y = self.ln_2(x)\n",
    "        y = self.mlp(y)\n",
    "        return x + y\n",
    "    \n",
    "mlp = MLP()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
